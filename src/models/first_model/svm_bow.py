"""
Preliminary code for submissions on the
Microsoft Malware Classification challenge.
"""

__authors__ = 'Aaron Gonzales, Andres Ruiz'
__licence__ = 'Apache'
__email__ = 'afruizc@cs.unm.edu'

import sys, os, argparse
import numpy as np
from sklearn import linear_model
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn.grid_search import GridSearchCV
from microsoft_malware_challenge.src.utils import utils
import joblib


class Executor(object):
    """
    Executes the selected classification pipeline. Right now the
    custmization process is by hand, i.e. you have to code it. The
    idea is to have a couple of pipelines prepared
    """

    def __init__(self):
        """
        Creates a new executor object and initializes the main
        components.
        """
        self.target_names = ['Ramnit', 'Lollipop', 'Kelihos_ver3',
                             'Vundo', 'Simda', 'Tracur', 'Kelihos_ver1',
                             'Obfuscator.ACY', 'Gatak']
        self.db = utils.get_mongodb()
        self.train = None
        self.test = None
        self.param_tunning = None
        self.fitted_model = None

    def _load_train(self):
        """
        Loads the training dataset.
        __THIS__ is the method you want to modify when querying
        the database.
        TODO: The data part can be just one function
        """
        data_train = [('\n'.join(x['ida_comments']), x['class'])
                          for x in self.db.samples.find({
                          "id": {"$exists": True}, # The id field exists
                          "ida_comments ": {"$ne": ""}})]
                            # ^ For non-empty field
        return list(zip(*data_train))

    def _load_test(self):
        """
        Loads the testing dataset.
        __THIS__ is the method you want to modify when querying
        the database.
        """
        data_test = [('\n'.join(x['ida_comments']), '"%s"' % x['id'])
                      for x in self.db.test_samples.find({
                            "id":{"$exists": True}, # The id field exists
                            "ida_comments": {"$ne": ""}})]
        return list(zip(*data_test))

    def load_data(self, training=True, testing=False):
        """
        Fetches the training data from the database. `training` and
        testing indicate the datasets that should be loaded.

        Arguments:
        `training`: If False, the training dataset is NOT loaded.
        `testng`: If True, the testing dataset IS loaded
        """
        if training:
            temp = self._load_train()
            self.train = {'data': (temp[0]), 'target': temp[1]}
        if testing:
            temp = self._load_test()
            self.test = {'data': (temp[0]), 'names': temp[1]}

    def config_model(self):
        """
        Configures the pipeline
        """
        pip = Pipeline([
                ('vectorizer', CountVectorizer(max_df=.8,
                               ngram_range=(1, 2))),
                ('freq_norm', TfidfTransformer()),
                ('classifier', linear_model.SGDClassifier(
                                    loss='modified_huber',
                                    penalty='elasticnet',
                                    alpha=1e-2,
                                    n_jobs=-1))
        ])
        parameters = {}
        self.param_tunning = GridSearchCV(pip, parameters, n_jobs=-1)

    def fit(self):
        """
        Fits the parameters to the pipeline
        """
        self.fitted_model = self.param_tunning.fit(self.train['data'],
                                                   self.train['target'])

    def _predict(self, X, create_submission=False, filename='submission.txt'):
        """
        Predicts a set of 9 probabilities per malware sample, that
        correspond to the 9 malware classes. If `create_submission`
        is True, then a text file named `filename` is created for
        submission into Kaggle.

        Arguments:
        `X`: The data in which predictions will be made.
        `create_submission`: Indicates whether a submission file should
        be created or not.
        `filename`: The file that will contain the submission.
        """
        predicted_prob = self.fitted_model.predict_proba(X)
        if create_submission:
            to_print = np.column_stack((np.array(self.test['names']),
                                        predicted_prob))
            np.savetxt(filename, to_print, header=','.join(['"id"'] + \
                                ['"Prediction%d"' % x for x in range(1, 10)]), \
                                   fmt='%s', delimiter=',')
        return predicted_prob

    def predict_on_test(self, create_submission=False,
                        filename='submission.txt'):
        """
        Performs predicton on the test dataset. see `_predict` for
        the Keyword arguments that can be used.

        Arguments:
        `**kwargs`: see `_predict`.
        """
        if self.test == None:
            sys.stderr.write("Test set not loaded. Aborting prediction\n")
            return
        return self._predict(self.test['data'], create_submission, filename)

    def load_model(self, filename='model.pkl'):
        """
        Attempts to load the already computed model from
        the `filename` file. If it is not found, then raises
        and exception.

        Argmuments:
        `filename`: The name of the file that contains the model
        """
        self.fitted_model = joblib.load(filename)

def config_parser():
    """
    Configures the parser for the command line arguments
    """
    parser = argparse.ArgumentParser()
    parser.add_argument('--save_model', default='model',
                                help='specifies the directory \
                                            where the model will be saved')
    return parser

def main():
    """
    Runs the main program
    """
    args = config_parser().parse_args()
    executor = Executor()
    print("Loading data...")
    executor.load_data(testing=True)
    print('Configuring the model...')
    executor.config_model()
    print('Fitting the model...')
    executor.fit()
    if not os.path.isdir(args.save_model):
        os.mkdir(args.save_model)
    save_path = os.path.join(args.save_model, 'model.pkl')
    joblib.dump(executor.fitted_model, save_path)
    print('Model saved on %s.' % save_path)
    print('Predicting...')
    executor.predict_on_test(create_submission=True)

if __name__ == '__main__':
    main()
