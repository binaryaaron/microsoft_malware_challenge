import gzip as gzip
import itertools
from collections import Counter
import os
import random
import itertools as it
import numpy as np
import src.utils.utils as utils
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.linear_model import Perceptron
import multiprocessing
from joblib import delayed, Parallel

def get_file(path, flat_string=False):
    """ Returns a single read bytecode file as a list of lists
    Args:
        path (string): full file path with filename and extension
    """
    with gzip.open(path+'.bytes.gz', 'rb') as f:
        # strips whitespace
        if flat_string is True:
            return ''.join([bytes.decode(line.strip()) for line in f.readlines()])
        else:
            return [bytes.decode(line.strip()) for line in f.readlines()]


def stream_files(collection, test=False):
    """
    Generator function to return a tuple of information about a file
    """
    if test:
        path = '/malware/data/test/'
        data = collection.find({'rand_id': {'$exists': True}},
                no_cursor_timeout=True)
    else:
        path = '/malware/data/train/'
        classes = [str(i) for i in range(10)]
        # required for keeping the cursor from timing out
        data = collection.find({'rand_id': {'$exists': True}},
                no_cursor_timeout=True)

    # number of classes in our data
    print(data)
    for i, doc in enumerate(data):
        if test:
            yield doc['id'], get_file(path + doc['id'])
        else:
            if i % 100 == 0:
                print('%d, about to get %s in class %s' % (i, doc['id'], doc['class']))
            yield doc['id'], doc['class'], get_file(path + doc['id'])


def tokenize(hexcode, bigrams=False):
    """
    Tokenizes a bytecode file surprisingly fast
    """
    # iterator over the split words
    i = iter(hexcode.split(' '))
    if bigrams is True:
        # magic. zip(i, i) makes a list of sequential word pair tuples
        return [' '.join(x) for x in zip(i, i)]
    if bigrams is False:
        return hexcode.split(' ')


def vectorize(hexcode, univocab, bivocab):
    """
    Vectorizes the words into python dicts to save in mongo
    """
    bigrams = tokenize(hexcode, bigrams=True)
    unigrams = tokenize(hexcode, bigrams=False)
    # makes this into a dictionary with 0 counts for all items
    uni_count = {k:0 for k in univocab}
    # .update addes the keys from the called dict to the old dict; replacing
    # keys. this ensures we have a uniform set of vectors.
    uni_count.update({x:count for x, count in Counter(unigrams).items()
                 if x in univocab})

    # makes this into a dictionary with 0 counts for all items
    bi_count = {k:0 for k in bivocab}
    bi_count.update({x:count for x, count in Counter(bigrams).items()
                if x in bivocab})
    return uni_count, bi_count


def get_hex_vocabs():
    """
    gets the range of 2-digit hexcodes as characters. range is 16^2, split is
    on the 'x' since python # prints them as 0x. we take just the matching
    hexcode, appending 00 and ??.

    Note that i am explicitly leaving them lowercase; scikit vectorizers change
    to lowercase anyway # and then we keep just the vocab of 2 digits or higher
    """
    uni_vocab = [i.split('x')[1].upper() for i in [hex(i) for i in range(16**2)]]
    uni_vocab.append('??')
    uni_vocab.append('00')
    uni_vocab = uni_vocab[16:]
    # all possible bigrams of 2-digit hexcodes, so take only
    # permuations of length 2, converting them to a single string 'FF 00' or similar
    bi_vocab = {str(a) + " " + str(b) for a, b in it.permutations(uni_vocab, 2)}
    return uni_vocab, bi_vocab


def extract_counts(doc_iter, test=False):
    """
    Main function for extracting the counts for all docs in the iterable
    Args:
        doc_iter (mongodb cursor)
        test (boolean): denotes if you want to do the test or train set
    """
    if test is True:
        path = '/malware/data/test/'
    else:
        path = '/malware/data/train/'

    uni_vocab, bi_vocab = get_hex_vocabs()
    # uses multiprocessing to speed up the process; it's trivially parallel
    Parallel(n_jobs=16)\
        (delayed(_extract_counts)(doc, path, uni_vocab, bi_vocab)
                for doc in doc_iter)


def _extract_counts(doc, path, uni_vocab, bi_vocab, debug=False):
    """Utility function that actually does the extraction
    Args:
    doc (mongodb object)
    path (string)
    uni_vocab (set): unigram vocabulary
    bi_vocab (set): bigram vocabular
    """
    print('vectorizing %s', doc['id'])
    hexcode = get_file(path + doc['id'], flat_string=True)
    uni_vec, bi_vec = vectorize(hexcode, uni_vocab, bi_vocab)
    if debug is True:
        return doc, uni_vec, bi_vec
    else:
        doc['hexcode'] = {}
        doc['hexcode']['unigrams'] = uni_vec
        doc['hexcode']['bigrams'] = bi_vec
        collection.save(doc)


def get_batch(mongo_iter, size):
    # limit size?

    data = [doc for doc in itertools.islice(mongo_iter, size)]
    X_id, X_lab, X_bytes = zip(*data)
    print("id %d, class, %d, data size %d" % (len(X_id), len(X_lab), len(X_bytes)))
    return X_bytes, np.asarray(X_lab, dtype=int)


def gen_batch(mongo_iter, batch_size=10):
    """
    Generator to return a batch of data
    """
    X_bytes, y = get_batch(mongo_iter, batch_size)
    while len(X_bytes):
        yield X_bytes, y
        X_bytes, y = get_batch(mongo_iter, batch_size)


def run(sample):
    db = utils.get_mongodb(username='populator')
    global collection
    test_val = False
    if sample == 'train':
        print('running on train set')
        collection = db.samples
        test_val = False
        docs = collection.find({'class': {"$exists": True}})
    if sample == 'test':
        print('running on test set')
        collection = db.test_samples
        test_val = True
        docs = collection.find({'id': {"$exists": True}})
    extract_counts(docs, test_val)


def run_partials(sample, testsize):
    doc_stream = stream_files()
    X_train_test, y_train_test = get_batch(doc_stream, testsize)
    h_vectorizer = make_vectorizer()
    bow_train_test = h_vectorizer.transform(X_train_test)
    print("Test set is %d documents " % (len(y_train_test),
        sum(y_train_test)))


def main():
    print('running on test set')
    run('test')

if __name__ == '__main__':
    main()
